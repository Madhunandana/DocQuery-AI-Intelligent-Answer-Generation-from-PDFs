# DocQuery AI - Chat with PDFs

**DocQuery AI** is an intelligent document processing application built with **Streamlit** and **LangChain**. It allows users to upload PDF documents, ask questions about their content, and receive detailed answers generated by **Google Gemini Pro**. The application also features a unique comparison mode that evaluates responses based on similarity and performance metrics.

## Features

*   **PDF Analysis**: Upload and process multiple PDF files simultaneously.
*   **Vector Search**: Uses **FAISS** (Facebook AI Similarity Search) and **Google Generative AI Embeddings** for efficient context retrieval.
*   **Intelligent Q&A**: Generates answers using the **Gemini Pro** Large Language Model (LLM).
*   **Model Comparison**: (Demonstration Feature) Compares two model configurations (labeled as Gemini and Codex) to analyze response similarity and generation time.
*   **Performance Metrics**: Visualizes response time and memory usage using **Matplotlib**.

## Prerequisites

*   Python 3.8+
*   **Google API Key** (for Gemini Pro and Embeddings)
*   **OpenAI API Key** (for configuration, though the current logic primarily uses Google models)

## Installation

1.  **Clone the repository** (if applicable):
    ```bash
    git clone <repository_url>
    cd ChatWithXandGmini
    ```

2.  **Create a virtual environment** (recommended):
    ```bash
    python -m venv venv
    # Windows
    venv\Scripts\activate
    # macOS/Linux
    source venv/bin/activate
    ```

3.  **Install dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

4.  **Environment Setup**:
    Create a `.env` file in the project root and add your API keys:
    ```env
    GOOGLE_API_KEY=your_google_api_key
    OPENAI_API_KEY=your_openai_api_key
    ```

## Usage

1.  **Run the Streamlit App**:
    ```bash
    streamlit run app.py
    ```

2.  **Interact with the Sidebar**:
    *   Navigate to the **Menu** in the sidebar.
    *   Click **"Upload your PDF Files"** and select one or more PDFs.
    *   Click **"Submit & Process"**. Wait for the "Done" success message.

3.  **Ask Questions**:
    *   Enter your question in the main text input field ("Intelligent Answer Generation from PDFs").
    *   The system will compare responses and display performance graphs.

## Code Explanation (`app.py`)

The application logic is contained within `app.py`. Here is a detailed breakdown of its components:

### 1. Libraries and Setup
*   **Streamlit**: Used for the web interface.
*   **PyPDF2**: Extracts text from PDF files.
*   **LangChain**: Framework for chaining LLM components (Embeddings, Vector Stores, Chat Models).
*   **Google Generative AI**: Accesses Gemini models.
*   **FAISS**: Handles vector storage and similarity search.

### 2. Core Functions

*   `get_pdf_text(pdf_docs)`:
    *   Iterates through uploaded PDF files.
    *   Extracts text from every page using `PdfReader`.
    *   Returns a single concatenated text string.

*   `get_text_chunks(text)`:
    *   Uses `RecursiveCharacterTextSplitter` to split the massive text into smaller, manageable chunks (10,000 characters) with overlap (1,000 characters). This ensures context is preserved across chunk boundaries for the embedding model.

*   `get_vector_store(text_chunks)`:
    *   Generates embeddings for each text chunk using `GoogleGenerativeAIEmbeddings` (`models/embedding-001`).
    *   Stores these vectors in a local `FAISS` index (`faiss_index`) for fast similarity search.

*   `get_conversational_chain()`:
    *   Configures a `ChatGoogleGenerativeAI` model (`gemini-pro`) with a `temperature` of 0.3 (creative but focused).
    *   Creates a `load_qa_chain` using a custom prompt template that instructs the model to answer from the provided context or state if the answer is unavailable.

*   `get_openai_codex_response(prompt)`:
    *   *Note: Despite the name suggesting OpenAI Codex, the current implementation is configured to use **Google Gemini Pro** with a lower temperature (0.2).*
    *   This functions as a secondary "expert" model for comparison purposes.

*   `calculate_similarity(response, reference)`:
    *   Uses `TfidfVectorizer` and `cosine_similarity` to mathematically compare how similar the two model responses are.

*   `user_input(user_question)`:
    *   **Orchestrator function**.
    *   Loads the FAISS index.
    *   Performs a similarity search to find PDF chunks relevant to the user's question.
    *   Generates a response using the primary chain (`get_conversational_chain`).
    *   If a valid answer is found:
        *   Generates a response using the secondary chain (`get_openai_codex_response`).
        *   Calculates similarity and performance metrics (Time, Memory).
        *   Plots these metrics using `matplotlib`.

### 3. Main Interface (`main`)
*   Sets up the Streamlit page config and header.
*   Handles the Sidebar file uploader.
*   Triggers the `user_input` function when a question is asked.

## Project Structure

```
ChatWithXandGmini/
├── app.py                # Main application script
├── requirements.txt      # Python dependencies
├── .env                  # API keys (not included in repo)
├── faiss_index/          # Local storage for vector embeddings
└── test_docquery_ai.py   # Unit tests
```

## Publication Link:(https://matjournals.net/engineering/index.php/JoIDACS/article/view/905)
<img width="1920" height="1080" alt="image" src="https://github.com/user-attachments/assets/d38022b8-136c-4fd7-a013-b9672f17096f" />


